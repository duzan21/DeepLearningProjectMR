{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Noam.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duzan21/DeepLearningProjectMR/blob/develop/Noam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDDHwlkITsiP",
        "cellView": "both"
      },
      "source": [
        "#@title Hyperparams\n",
        "\n",
        "networkType = \"CNN2\"#@param [\"CNN2\",\"CNN2_FFT\", \"CNN2_ENSEMBLE\", \"VGG\", \"VGG_ENSEMBLE\"]\n",
        "dataSet = 'RML2016.10a_dict'#@param [\"RML2016.10a_dict\", \"2016.04C.multisnr\"]\n",
        "loadTrainedModel =  True #@param {type:\"boolean\"}\n",
        "action = \"Analyze model history\" #@param [\"Show trained results\", \"Analyze model history\"]\n",
        "\n",
        "learningRate = 0.0002 #@param {type:\"slider\", min:0.0001, max:0.005, step:0.0001}\n",
        "num_epochs = 1#@param {type:\"slider\", min:1, max:200, step:1}\n",
        "epochStart = 0\n",
        "dropout = 0.5 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "batch_size =  1024#@param {type:\"integer\"}\n",
        "\n",
        "rootDir = '/content/drive/My Drive/Deep Learning course 2020/Proj/'\n",
        "SEED =  203142260 + 305773673\n",
        "\n",
        "isVGG = False\n",
        "isEnsemble = False\n",
        "if networkType is not \"CNN2_FFT\":\n",
        "  if networkType is \"VGG\":\n",
        "    isVGG = True\n",
        "  if \"ENSEMBLE\" in networkType:\n",
        "    isEnsemble = True\n",
        "\n",
        "vggChannels = 1024\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJRDSVrY7uGv",
        "cellView": "both",
        "outputId": "5dc8017b-6e1c-43e3-b4a4-5fce3c2fb4c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title imports and mount google drive\n",
        "\n",
        "%matplotlib inline\n",
        "import os,random,sys\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "device = torch.device(\"cuda\") \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from glob import glob\n",
        "import re\n",
        "\n",
        "# In Python 2, cPickle is the accelerated version of pickle, and a later addition to thestandard library.\n",
        "# It was perfectly normal to import it with a fallback to pickle. In Python 3 the accelerated version has been integrated\n",
        "# and there is simply no reason to use anything other than import pickle\n",
        "import pickle\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "root = rootDir\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyD4scXw7uG2"
      },
      "source": [
        "# Dataset setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYz8QqbJ7uG3",
        "cellView": "both",
        "outputId": "fd134f39-854a-42c1-bff2-674fafaf5467",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "source": [
        "#@title Dataset setup and FFT preprocess\n",
        "\n",
        "def IQ2Complex(iqArray):\n",
        "  return iqArray[:,0,:] + 1j*iqArray[:,1,:]\n",
        "def Complex2IQ(complexArray):\n",
        "  I = np.real(complexArray)\n",
        "  Q = np.imag(complexArray)\n",
        "  return np.concatenate((I[:,np.newaxis,:], Q[:,np.newaxis,:]), axis=1)\n",
        "\n",
        "# Load the dataset ...\n",
        "Xd = pickle.load(open(root + dataSet + \".pkl\",'rb'), encoding=\"latin1\")\n",
        "snrs,mods = map(lambda j: sorted(list(set(map(lambda x: x[j], Xd.keys())))), [1,0])\n",
        "X = []  \n",
        "lbl = []\n",
        "for mod in mods:\n",
        "    for snr in snrs:\n",
        "        X.append(Xd[(mod,snr)])\n",
        "        for i in range(Xd[(mod,snr)].shape[0]):  lbl.append((mod,snr))\n",
        "X = np.vstack(X)\n",
        "XComplex = IQ2Complex(X)\n",
        "fftXComplex = np.fft.fft(XComplex)\n",
        "fftX_IQ = np.array(Complex2IQ(fftXComplex), dtype=np.float32)\n",
        "test_SNRs = np.array(list(map(lambda x: lbl[x][1], test_idx)))\n",
        "\n",
        "if networkType is \"CNN2_FFT\":\n",
        "  X = np.concatenate((X, fftX_IQ), axis=1)\n",
        "print(X.shape)\n",
        "print(fftX_IQ.shape)\n",
        "print(X.dtype)\n",
        "print(fftX_IQ.dtype)\n",
        "print(Counter(lbl))\n",
        "# X: (N, 2, 128). for each of the N=220,000 examples, there is a 128-long measurement of I/Q samples.\n",
        "# mods: ['8PSK', 'AM-DSB', 'AM-SSB', 'BPSK', 'CPFSK', 'GFSK', 'PAM4', 'QAM16', 'QAM64', 'QPSK', 'WBFM']\n",
        "# lbl: list of tuples, each of the pattern ('GFSK', -20)\n",
        "\n",
        "counterLbl = Counter(lbl);\n",
        "for snr in [snrs[0]]:\n",
        "  lblSnr = {}\n",
        "  for mod in mods:\n",
        "    lblSnr[mod] = counterLbl[(mod, snr)]\n",
        "  print(lblSnr)\n",
        "  ticks = range(0, 10*len(lblSnr.values()), 10)\n",
        "  plt.figure(figsize=(10, 3))  # width:10, height:8\n",
        "  plt.bar(ticks,lblSnr.values(), align='center', width=1, )\n",
        "  plt.xticks(ticks, lblSnr.keys())\n",
        "  plt.show()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(220000, 2, 128)\n",
            "(220000, 2, 128)\n",
            "float32\n",
            "float32\n",
            "Counter({('8PSK', -20): 1000, ('8PSK', -18): 1000, ('8PSK', -16): 1000, ('8PSK', -14): 1000, ('8PSK', -12): 1000, ('8PSK', -10): 1000, ('8PSK', -8): 1000, ('8PSK', -6): 1000, ('8PSK', -4): 1000, ('8PSK', -2): 1000, ('8PSK', 0): 1000, ('8PSK', 2): 1000, ('8PSK', 4): 1000, ('8PSK', 6): 1000, ('8PSK', 8): 1000, ('8PSK', 10): 1000, ('8PSK', 12): 1000, ('8PSK', 14): 1000, ('8PSK', 16): 1000, ('8PSK', 18): 1000, ('AM-DSB', -20): 1000, ('AM-DSB', -18): 1000, ('AM-DSB', -16): 1000, ('AM-DSB', -14): 1000, ('AM-DSB', -12): 1000, ('AM-DSB', -10): 1000, ('AM-DSB', -8): 1000, ('AM-DSB', -6): 1000, ('AM-DSB', -4): 1000, ('AM-DSB', -2): 1000, ('AM-DSB', 0): 1000, ('AM-DSB', 2): 1000, ('AM-DSB', 4): 1000, ('AM-DSB', 6): 1000, ('AM-DSB', 8): 1000, ('AM-DSB', 10): 1000, ('AM-DSB', 12): 1000, ('AM-DSB', 14): 1000, ('AM-DSB', 16): 1000, ('AM-DSB', 18): 1000, ('AM-SSB', -20): 1000, ('AM-SSB', -18): 1000, ('AM-SSB', -16): 1000, ('AM-SSB', -14): 1000, ('AM-SSB', -12): 1000, ('AM-SSB', -10): 1000, ('AM-SSB', -8): 1000, ('AM-SSB', -6): 1000, ('AM-SSB', -4): 1000, ('AM-SSB', -2): 1000, ('AM-SSB', 0): 1000, ('AM-SSB', 2): 1000, ('AM-SSB', 4): 1000, ('AM-SSB', 6): 1000, ('AM-SSB', 8): 1000, ('AM-SSB', 10): 1000, ('AM-SSB', 12): 1000, ('AM-SSB', 14): 1000, ('AM-SSB', 16): 1000, ('AM-SSB', 18): 1000, ('BPSK', -20): 1000, ('BPSK', -18): 1000, ('BPSK', -16): 1000, ('BPSK', -14): 1000, ('BPSK', -12): 1000, ('BPSK', -10): 1000, ('BPSK', -8): 1000, ('BPSK', -6): 1000, ('BPSK', -4): 1000, ('BPSK', -2): 1000, ('BPSK', 0): 1000, ('BPSK', 2): 1000, ('BPSK', 4): 1000, ('BPSK', 6): 1000, ('BPSK', 8): 1000, ('BPSK', 10): 1000, ('BPSK', 12): 1000, ('BPSK', 14): 1000, ('BPSK', 16): 1000, ('BPSK', 18): 1000, ('CPFSK', -20): 1000, ('CPFSK', -18): 1000, ('CPFSK', -16): 1000, ('CPFSK', -14): 1000, ('CPFSK', -12): 1000, ('CPFSK', -10): 1000, ('CPFSK', -8): 1000, ('CPFSK', -6): 1000, ('CPFSK', -4): 1000, ('CPFSK', -2): 1000, ('CPFSK', 0): 1000, ('CPFSK', 2): 1000, ('CPFSK', 4): 1000, ('CPFSK', 6): 1000, ('CPFSK', 8): 1000, ('CPFSK', 10): 1000, ('CPFSK', 12): 1000, ('CPFSK', 14): 1000, ('CPFSK', 16): 1000, ('CPFSK', 18): 1000, ('GFSK', -20): 1000, ('GFSK', -18): 1000, ('GFSK', -16): 1000, ('GFSK', -14): 1000, ('GFSK', -12): 1000, ('GFSK', -10): 1000, ('GFSK', -8): 1000, ('GFSK', -6): 1000, ('GFSK', -4): 1000, ('GFSK', -2): 1000, ('GFSK', 0): 1000, ('GFSK', 2): 1000, ('GFSK', 4): 1000, ('GFSK', 6): 1000, ('GFSK', 8): 1000, ('GFSK', 10): 1000, ('GFSK', 12): 1000, ('GFSK', 14): 1000, ('GFSK', 16): 1000, ('GFSK', 18): 1000, ('PAM4', -20): 1000, ('PAM4', -18): 1000, ('PAM4', -16): 1000, ('PAM4', -14): 1000, ('PAM4', -12): 1000, ('PAM4', -10): 1000, ('PAM4', -8): 1000, ('PAM4', -6): 1000, ('PAM4', -4): 1000, ('PAM4', -2): 1000, ('PAM4', 0): 1000, ('PAM4', 2): 1000, ('PAM4', 4): 1000, ('PAM4', 6): 1000, ('PAM4', 8): 1000, ('PAM4', 10): 1000, ('PAM4', 12): 1000, ('PAM4', 14): 1000, ('PAM4', 16): 1000, ('PAM4', 18): 1000, ('QAM16', -20): 1000, ('QAM16', -18): 1000, ('QAM16', -16): 1000, ('QAM16', -14): 1000, ('QAM16', -12): 1000, ('QAM16', -10): 1000, ('QAM16', -8): 1000, ('QAM16', -6): 1000, ('QAM16', -4): 1000, ('QAM16', -2): 1000, ('QAM16', 0): 1000, ('QAM16', 2): 1000, ('QAM16', 4): 1000, ('QAM16', 6): 1000, ('QAM16', 8): 1000, ('QAM16', 10): 1000, ('QAM16', 12): 1000, ('QAM16', 14): 1000, ('QAM16', 16): 1000, ('QAM16', 18): 1000, ('QAM64', -20): 1000, ('QAM64', -18): 1000, ('QAM64', -16): 1000, ('QAM64', -14): 1000, ('QAM64', -12): 1000, ('QAM64', -10): 1000, ('QAM64', -8): 1000, ('QAM64', -6): 1000, ('QAM64', -4): 1000, ('QAM64', -2): 1000, ('QAM64', 0): 1000, ('QAM64', 2): 1000, ('QAM64', 4): 1000, ('QAM64', 6): 1000, ('QAM64', 8): 1000, ('QAM64', 10): 1000, ('QAM64', 12): 1000, ('QAM64', 14): 1000, ('QAM64', 16): 1000, ('QAM64', 18): 1000, ('QPSK', -20): 1000, ('QPSK', -18): 1000, ('QPSK', -16): 1000, ('QPSK', -14): 1000, ('QPSK', -12): 1000, ('QPSK', -10): 1000, ('QPSK', -8): 1000, ('QPSK', -6): 1000, ('QPSK', -4): 1000, ('QPSK', -2): 1000, ('QPSK', 0): 1000, ('QPSK', 2): 1000, ('QPSK', 4): 1000, ('QPSK', 6): 1000, ('QPSK', 8): 1000, ('QPSK', 10): 1000, ('QPSK', 12): 1000, ('QPSK', 14): 1000, ('QPSK', 16): 1000, ('QPSK', 18): 1000, ('WBFM', -20): 1000, ('WBFM', -18): 1000, ('WBFM', -16): 1000, ('WBFM', -14): 1000, ('WBFM', -12): 1000, ('WBFM', -10): 1000, ('WBFM', -8): 1000, ('WBFM', -6): 1000, ('WBFM', -4): 1000, ('WBFM', -2): 1000, ('WBFM', 0): 1000, ('WBFM', 2): 1000, ('WBFM', 4): 1000, ('WBFM', 6): 1000, ('WBFM', 8): 1000, ('WBFM', 10): 1000, ('WBFM', 12): 1000, ('WBFM', 14): 1000, ('WBFM', 16): 1000, ('WBFM', 18): 1000})\n",
            "{'8PSK': 1000, 'AM-DSB': 1000, 'AM-SSB': 1000, 'BPSK': 1000, 'CPFSK': 1000, 'GFSK': 1000, 'PAM4': 1000, 'QAM16': 1000, 'QAM64': 1000, 'QPSK': 1000, 'WBFM': 1000}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQfhV5JS7uHE"
      },
      "source": [
        "# Build the NN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5G7pwT347uHF",
        "cellView": "form",
        "outputId": "f50a9d22-424c-4cd5-d336-bff931f2fe50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "#@title setting up the network\n",
        "#Pytorch\n",
        "dr = dropout\n",
        "class CNN2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN2, self).__init__()\n",
        "        self.convPart = nn.Sequential(\n",
        "            # input is batch x 1 x 2 x 128\n",
        "            # (Reshape([1]+in_shp, input_shape=in_shp))  #  - Reshape [N,2,128] to [N,1,2,128] on input\n",
        "            nn.Conv2d(1, 256, kernel_size=(1, 3), padding=(0, 2)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(dr),\n",
        "            # state size. batch x 256 x 2 x 130\n",
        "            nn.Conv2d(in_channels=256, out_channels=80, kernel_size=(2, 3), padding=(0, 2)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(dr),\n",
        "            # state size. batch x 80 x 1 x 132\n",
        "        )\n",
        "        \n",
        "        self.firstLinearSize = 80 * 132\n",
        "        self.linPart = nn.Sequential(\n",
        "            nn.Linear(self.firstLinearSize, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(dr),\n",
        "            # state size. batch x 256\n",
        "            nn.Linear(256, len(mods)),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = self.convPart(input)\n",
        "        out = out.view(-1, self.firstLinearSize)\n",
        "        out = self.linPart(out)\n",
        "        #out = F.softmax(out, dim=1)\n",
        "        return out\n",
        "\n",
        "class CNN2_FFT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN2_FFT, self).__init__()\n",
        "        self.convPart = nn.Sequential(\n",
        "            # input is batch x 1 x 4 x 128\n",
        "            # (Reshape([1]+in_shp, input_shape=in_shp))  #  - Reshape [N,2,128] to [N,1,2,128] on input\n",
        "            nn.Conv2d(1, 256, kernel_size=(1, 3), padding=(0, 2)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(dr),\n",
        "            # state size. batch x 256 x 2 x 130\n",
        "            nn.Conv2d(in_channels=256, out_channels=80, kernel_size=(3, 3), padding=(0, 2)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(dr),\n",
        "            # state size. batch x 80 x 2 x 132\n",
        "        )\n",
        "        \n",
        "        self.firstLinearSize = 80 * 132 * 2\n",
        "        self.linPart = nn.Sequential(\n",
        "            nn.Linear(self.firstLinearSize, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(dr),\n",
        "            # state size. batch x 256\n",
        "            nn.Linear(256, len(mods)),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = self.convPart(input)\n",
        "        out = out.view(-1, self.firstLinearSize)\n",
        "        out = self.linPart(out)\n",
        "        #out = F.softmax(out, dim=1)\n",
        "        return out\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG, self).__init__()\n",
        "        channels = vggChannels\n",
        "        self.convPart = nn.Sequential(\n",
        "            # input is batch x 1 x 2 x 128\n",
        "            # (Reshape([1]+in_shp, input_shape=in_shp))  #  - Reshape [N,2,128] to [N,1,2,128] on input\n",
        "            nn.Conv2d(in_channels=1, out_channels=channels, kernel_size=(2, 3), padding=(0, 1)),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d((1,2),(1,2)),\n",
        "            nn.Dropout(dr),\n",
        "            nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=(1, 3), padding=(0, 1)),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d((1,2),(1,2)),\n",
        "            nn.Dropout(dr),\n",
        "            nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=(1, 3), padding=(0, 1)),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d((1,2),(1,2)),\n",
        "            nn.Dropout(dr),\n",
        "            nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=(1, 3), padding=(0, 1)),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d((1,2),(1,2)),\n",
        "            nn.Dropout(dr)\n",
        "            # state size. batch x channels x 1 x 8\n",
        "            )\n",
        "        \n",
        "        self.firstLinearSize = channels * 8\n",
        "        self.linPart = nn.Sequential(\n",
        "            nn.Linear(self.firstLinearSize, 128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(dr),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(dr),\n",
        "            # state size. batch x 256\n",
        "            nn.Linear(128, len(mods)),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = self.convPart(input)\n",
        "        out = out.view(-1, self.firstLinearSize)\n",
        "        out = self.linPart(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class Ensemble(nn.Module):\n",
        "    def __init__(self, isVGG = False):\n",
        "        super(Ensemble, self).__init__()\n",
        "        if isVGG == True:\n",
        "          self.modelTimeSamples = VGG()\n",
        "          self.modelFFTSamples = VGG()\n",
        "        else:\n",
        "          self.modelTimeSamples = CNN2()\n",
        "          self.modelFFTSamples = CNN2()\n",
        "        self.linPart = nn.Sequential(\n",
        "            nn.Linear(2*len(mods), len(mods)),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = self.modelTimeSamples(input[:,0])\n",
        "        outFFT = self.modelFFTSamples(input[:,1]) \n",
        "        out = torch.cat((out, outFFT), 1)\n",
        "        out = out.view(-1, 2*len(mods))\n",
        "        out = self.linPart(out)\n",
        "        #out = F.softmax(out, dim=1)\n",
        "        return out\n",
        "\n",
        "# custom weights initialization #  to mean=0, stdev=0.2.\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "    elif classname.find('Linear') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "# Create the generator and the Discriminator\n",
        "net =  CNN2_FFT().to(device)\n",
        "if networkType is not \"CNN2_FFT\":\n",
        "  if isVGG:\n",
        "    net =  VGG().to(device)\n",
        "  else:\n",
        "    net =  CNN2().to(device)\n",
        "  if isEnsemble:\n",
        "    net =  Ensemble(isVGG).to(device)\n",
        "# Apply the weights_init function to randomly initialize all weights\n",
        "net.apply(weights_init)\n",
        "\n",
        "# Print the model                                    \n",
        "print(net)\n",
        "\n",
        "# model.add(Reshape([len(classes)]))\n",
        "# model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "if isEnsemble:\n",
        "  summary(net, (2, 1, 2, 128))\n",
        "else:\n",
        "  if networkType is \"CNN2_FFT\":\n",
        "    summary(net, (1, 4, 128))\n",
        "  else:\n",
        "    summary(net, (1, 2, 128))\n",
        "\n",
        "lr = learningRate\n",
        "beta1 = 0.5\n",
        "beta2 = 0.999\n",
        "\n",
        "#criterion = nn.CrossEntropyLoss()\n",
        "criterion = nn.NLLLoss()\n",
        "# Setup Adam optimizers for both G and D\n",
        "optimizer = optim.Adam(net.parameters(), lr=lr, betas=(beta1, beta2))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN2(\n",
            "  (convPart): Sequential(\n",
            "    (0): Conv2d(1, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Conv2d(256, 80, kernel_size=(2, 3), stride=(1, 1), padding=(0, 2))\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (linPart): Sequential(\n",
            "    (0): Linear(in_features=10560, out_features=256, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=256, out_features=11, bias=True)\n",
            "    (4): LogSoftmax(dim=1)\n",
            "  )\n",
            ")\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 256, 2, 130]           1,024\n",
            "              ReLU-2          [-1, 256, 2, 130]               0\n",
            "           Dropout-3          [-1, 256, 2, 130]               0\n",
            "            Conv2d-4           [-1, 80, 1, 132]         122,960\n",
            "              ReLU-5           [-1, 80, 1, 132]               0\n",
            "           Dropout-6           [-1, 80, 1, 132]               0\n",
            "            Linear-7                  [-1, 256]       2,703,616\n",
            "              ReLU-8                  [-1, 256]               0\n",
            "           Dropout-9                  [-1, 256]               0\n",
            "           Linear-10                   [-1, 11]           2,827\n",
            "       LogSoftmax-11                   [-1, 11]               0\n",
            "================================================================\n",
            "Total params: 2,830,427\n",
            "Trainable params: 2,830,427\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.77\n",
            "Params size (MB): 10.80\n",
            "Estimated Total Size (MB): 12.57\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75WvJcjT7uHM"
      },
      "source": [
        "# Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcS_0PuxtPSa",
        "cellView": "form"
      },
      "source": [
        "#@title Training helper functions\n",
        "def prepare_data_into_batches(data, labels, permute, isEnsemble = False):\n",
        "  # Prepare data\n",
        "  if permute is True:\n",
        "    perm = torch.randperm(len(data))  # Shuffles along the first axis. shape = (tot_size, 2, 128) just like X_train\n",
        "  else:\n",
        "    perm = np.arange(len(data))\n",
        "\n",
        "  n_batches = len(data) // batch_size\n",
        "  tot_size = n_batches * batch_size\n",
        "  \n",
        "  shuffled = data[perm]\n",
        "  shuffled = shuffled[:tot_size]        # Cutting off the last part that is smaller than one batch.  shape = (tot_size, 2, 128) just like X_train\n",
        "  if isEnsemble:\n",
        "    prepared = shuffled.reshape(n_batches, -1, 2, 2, 128)  # Get into a shape that each element is a batch. \n",
        "    prepared = np.expand_dims(prepared, axis=3)           # Add a space for num_channels (it's 1 in our case).\n",
        "  else:\n",
        "    if networkType == \"CNN2_FFT\":\n",
        "      prepared = shuffled.reshape(n_batches, -1, 4, 128)\n",
        "    else:\n",
        "      prepared = shuffled.reshape(n_batches, -1, 2, 128)  # Get into a shape that each element is a batch. \n",
        "    prepared = np.expand_dims(prepared, axis=2)           # Add a space for num_channels (it's 1 in our case). \n",
        "\n",
        "  labels = labels[perm]\n",
        "  labels = labels[:tot_size]\n",
        "  labels = labels.reshape(n_batches, -1)\n",
        "  return prepared, labels\n",
        "  \n",
        "def get_batch_results(batch):\n",
        "  batchData = torch.from_numpy(batch[0]).to(device)\n",
        "  batchLabels = torch.from_numpy(batch[1]).to(device)\n",
        "  \n",
        "  output = net(batchData)\n",
        "  _, pred = torch.max(output.detach(), dim=1)\n",
        "  correct = torch.sum(pred.eq(batchLabels)).cpu().numpy()\n",
        "\n",
        "  return output, batchLabels, correct\n",
        "\n",
        "  \n",
        "def save_network(epoch):\n",
        "  path = f'{root}saved models/{networkType}/{networkType}_{dataSet}__epoch {epoch+1}.pth'\n",
        "  torch.save(netD.state_dict(), path)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVanAyA0q4pp",
        "cellView": "form"
      },
      "source": [
        "#@title Network training loop\n",
        "def train_network():\n",
        "  trainLoss = np.zeros(num_epochs)\n",
        "  testLoss = np.zeros(num_epochs)\n",
        "\n",
        "  startEpoch = 0\n",
        "  #loadPath = f'{root}saved models/epoch {100}.pt'\n",
        "  #filename = glob(loadPath)[0]\n",
        "  #net.load_state_dict(torch.load(filename))\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    totEpoch = epoch + startEpoch\n",
        "    print(f'epoch: {totEpoch + 1}/{num_epochs + startEpoch}, ', end='')\n",
        "    start = time.time()\n",
        "\n",
        "    trainData, trainLables = prepare_data_into_batches(X_train, Y_train, permute=True, isEnsemble=isEnsemble)\n",
        "    testData, testLables = prepare_data_into_batches(X_test, Y_test, permute=True, isEnsemble=isEnsemble)\n",
        "    \n",
        "    net.train()\n",
        "    correctTrain = 0\n",
        "    trainEpochloss = 0\n",
        "    for i, batch in enumerate(zip(trainData, trainLables)):\n",
        "      net.zero_grad()\n",
        "\n",
        "      output, batchLabels, correct = get_batch_results(batch)\n",
        "      loss = criterion(output, batchLabels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      correctTrain += correct\n",
        "      trainEpochloss += loss\n",
        "\n",
        "    net.eval()\n",
        "    correctTest = 0\n",
        "    testEpochloss = 0\n",
        "    with torch.no_grad():\n",
        "      for i, batch in enumerate(zip(testData, testLables)):\n",
        "        output, batchLabels, correct = get_batch_results(batch)\n",
        "        correctTest += correct\n",
        "        testEpochloss += criterion(output, batchLabels)\n",
        "\n",
        "    trainAcc = correctTrain / epoch_size * 100\n",
        "    testAcc = correctTest / epoch_size * 100\n",
        "    trainLoss[epoch] = trainEpochloss / num_batches\n",
        "    testLoss[epoch] = testEpochloss / num_batches\n",
        "    print(f'train loss: {trainLoss[epoch]:.2f}, test loss: {testLoss[epoch]:.2f}, train accuracy: {trainAcc:.2f}%, test accuracy: {testAcc:.2f}%, elapsed time: {time.time() - start:.0f}s')\n",
        "    \n",
        "    # torch.save(net.state_dict(), f'{root}saved models/{networkType}/{networkType}_epoch{totEpoch+1}.pth')\n",
        "    return trainLoss, testLoss"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYkEUf7CfaZa",
        "cellView": "form"
      },
      "source": [
        "#@title Train network!\n",
        "trainData, trainLables = prepare_data_into_batches(X_train, Y_train, permute=True, isEnsemble=isEnsemble)\n",
        "# print(trainData.shape)\n",
        "if loadTrainedModel is False:\n",
        "  trainLoss, testLoss = train_network()\n",
        "    # Show loss curves \n",
        "  plt.figure()\n",
        "  plt.title('Training performance')\n",
        "  plt.plot(trainLoss, label='train loss') \n",
        "  plt.plot(testLoss, label='test loss') \n",
        "  plt.legend()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6osgpBt7uHR"
      },
      "source": [
        "# Evaluate and Plot Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2S062N17uHb",
        "cellView": "form"
      },
      "source": [
        "#@title Confusion matrix helper functions\n",
        "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues, labels=[]):\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(labels))\n",
        "    plt.xticks(tick_marks, labels, rotation=45)\n",
        "    plt.yticks(tick_marks, labels)\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "\n",
        "def get_conf_from_batch(out, labels):\n",
        "  batch_conf = np.zeros([len(mods),len(mods)], dtype=int)\n",
        "  for i in range(len(labels)):\n",
        "    j = labels[i]              # true label\n",
        "    k = int(np.argmax(out[i]))   # predicted label\n",
        "    batch_conf[j,k] += 1\n",
        "  return batch_conf\n",
        "\n",
        "\n",
        "def prepare_batches(snr):\n",
        "    snrIndices = np.argwhere(test_SNRs==snr)\n",
        "    batches_per_snr = len(snrIndices) // batch_size\n",
        "    last_batch_size = np.remainder(len(snrIndices), batch_size)\n",
        "    \n",
        "    snr_x = X_test[snrIndices]\n",
        "    snr_y = Y_test[snrIndices]    \n",
        "\n",
        "    signals, lables = prepare_data_into_batches(snr_x, snr_y, permute=False, isEnsemble=isEnsemble)\n",
        "    \n",
        "    x_last_batch = snr_x[-last_batch_size:]\n",
        "    y_last_batch = snr_y[-last_batch_size:]\n",
        "    # print(x_last_batch.shape)\n",
        "    if isEnsemble:\n",
        "      x_last_batch = x_last_batch.reshape(-1, 2, 2, 128)  \n",
        "      x_last_batch = np.expand_dims(x_last_batch, axis=2) \n",
        "    else:\n",
        "      if networkType is \"CNN2_FFT\":\n",
        "        x_last_batch = x_last_batch.reshape(-1, 4, 128)  \n",
        "      else:\n",
        "        x_last_batch = x_last_batch.reshape(-1, 2, 128)\n",
        "      x_last_batch = np.expand_dims(x_last_batch, axis=1)          \n",
        "    # print(x_last_batch.shape)       \n",
        "    # y_last_batch = y_last_batch.reshape(-1)\n",
        "    \n",
        "    return signals, lables, x_last_batch, y_last_batch\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "8hKr8pIR7uHl",
        "cellView": "form"
      },
      "source": [
        "#@title Model analysis helper functions\n",
        "def get_model_performance(plot_conf = False, save_path=''):\n",
        "  acc = {}\n",
        "  net.eval()\n",
        "  for snr in snrs:\n",
        "    # print(f'SNR = {snr}dB: ', end='')\n",
        "    conf = np.zeros([len(mods),len(mods)], dtype=int)\n",
        "    confnorm = np.zeros([len(mods),len(mods)], dtype=float)\n",
        "    signals, lables, x_last_batch, y_last_batch = prepare_batches(snr)\n",
        "\n",
        "    for batch in zip(signals, lables):\n",
        "      output, batchLabels,_ = get_batch_results(batch)\n",
        "      conf += get_conf_from_batch(output.detach().cpu().numpy(), batchLabels)\n",
        "    # last partial batch\n",
        "    output, batchLabels,_ = get_batch_results((x_last_batch, y_last_batch))\n",
        "    conf += get_conf_from_batch(output.detach().cpu().numpy(), batchLabels)\n",
        "\n",
        "    for i in range(len(mods)):\n",
        "      row_sum = np.sum(conf[i,:])\n",
        "      if row_sum != 0:\n",
        "        confnorm[i,:] = conf[i,:] / row_sum\n",
        "\n",
        "    if plot_conf is True:  \n",
        "      plt.figure()\n",
        "      plot_confusion_matrix(confnorm, labels=mods, title=f'ConvNet Confusion Matrix (SNR={snr})')\n",
        "      if os.path.isdir(save_path) is False:\n",
        "        os.mkdir(save_path)\n",
        "      plt.savefig(f'{save_path}SNR: {snr}dB')\n",
        "    cor = np.sum(np.diag(confnorm))\n",
        "    ncor = np.sum(confnorm) - cor\n",
        "    acc[snr] = cor/(cor+ncor) * 100\n",
        "  return acc\n",
        "\n",
        "\n",
        "  def plot_accuracy_graph(acc, save_fig = False, save_path='', legend=[]):\n",
        "  plt.figure()\n",
        "  if len(legend) >= 2:\n",
        "    assert len(acc) == len(legend)\n",
        "    for model in acc:\n",
        "      plt.plot(list(model.keys()), list(model.values()))\n",
        "      plt.legend(legend)\n",
        "  else:\n",
        "    plt.plot(list(acc.keys()), list(acc.values()))\n",
        "  plt.grid()\n",
        "  plt.title(f'model: {networkType}')\n",
        "  plt.xlabel(\"SNR [dB]\")\n",
        "  plt.ylabel(\"Classification Accuracy [%]\")\n",
        "\n",
        "  if save_fig is True:\n",
        "    plt.savefig(save_path)\n",
        "\n",
        "\n",
        "def analyze_model_history():\n",
        "  loadPath = f'{root}saved models/{networkType}/'#{networkType}_{dataSet}__epoch {epoch+1}.pth'\n",
        "  file_names = [f for f in os.listdir(loadPath) if os.path.isfile(os.path.join(loadPath, f)) and f.split('epoch')[0] != f and dataSet in f]\n",
        "  data_name = 'new data' if dataSet ==\"RML2016.10a_dict\" else 'old data'\n",
        "\n",
        "  # if the model has already been analyzed - return\n",
        "  stats_path = f'{root}saved results/{networkType}, {data_name}'\n",
        "  if os.path.isfile(stats_path):\n",
        "    return\n",
        "\n",
        "  # Go over all the models, evaluate them and save pickle\n",
        "  max_epoch = 0\n",
        "  for f in file_names:\n",
        "    max_epoch = max(max_epoch, int(f.split('epoch')[-1].split('.')[0]))\n",
        "  \n",
        "  acc = [None] * max_epoch\n",
        "  for i, f in enumerate(file_names):\n",
        "    epoch_num = int(f.split('epoch')[-1].split('.')[0])\n",
        "    if i % 10 == 0:\n",
        "      print(f'Model #: {i+1}/{len(file_names)}, epoch #: {epoch_num}')\n",
        "\n",
        "    net.load_state_dict(torch.load(f'{loadPath}{networkType}_{dataSet}_epoch{epoch_num}.pth'))\n",
        "    acc[epoch_num - 1] = get_model_performance(plot_conf=False)\n",
        "\n",
        "  fd = open(stats_path,'wb')\n",
        "  pickle.dump(acc , fd)\n",
        "  fd.close()\n",
        "\n",
        "\n",
        "def show_trained_results():\n",
        "  loadPath = f'{root}saved models/{networkType}/'#{networkType}_{dataSet}__epoch {epoch+1}.pth'\n",
        "  file_names = [f for f in os.listdir(loadPath) if os.path.isfile(os.path.join(loadPath, f)) and f.split('epoch')[0] != f and dataSet in f]\n",
        "  data_name = 'new data' if dataSet ==\"RML2016.10a_dict\" else 'old data'\n",
        "\n",
        "  # Go over all the models, evaluate them and save pickle\n",
        "  max_epoch = 0\n",
        "  for f in file_names:\n",
        "    max_epoch = max(max_epoch, int(f.split('epoch')[-1].split('.')[0]))\n",
        "  \n",
        "  title = f'model: {networkType}, {data_name}'\n",
        "  snr_path = f'{root}saved results/snr graphs/{title}'\n",
        "  conf_path = f'{root}saved results/conf graphs/{title}/'\n",
        "  model_path = f'{loadPath}{networkType}_{dataSet}_epoch{max_epoch}.pth'\n",
        "\n",
        "  # Show the results of the last epoch\n",
        "  net.load_state_dict(torch.load(model_path))    \n",
        "  acc = get_model_performance(plot_conf=True, save_path=conf_path)\n",
        "  plot_accuracy_graph(acc, save_fig=True, save_path=snr_path)\n",
        "\n",
        "  detection_th = snrs[min(np.argwhere(np.array(list(acc.values())) > (50+100/22)))[0]]\n",
        "  print(f'First snr better than 54.5% is: {detection_th}dB, accuracy at SNR=-6dB: {acc[-6]:.2f}%')\n",
        "\n",
        "    \n",
        " "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPqcxjXnisl-",
        "cellView": "form"
      },
      "source": [
        "#@title Perform model analysis\n",
        "if loadTrainedModel is True:\n",
        "  dataSet = \"RML2016.10a_dict\"\n",
        "  if action == \"Analyze model history\":\n",
        "    analyze_model_history()\n",
        "  if action == \"Show trained results\":\n",
        "    show_trained_results()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUezT4CRv0na",
        "cellView": "form"
      },
      "source": [
        "#@title Analyze all saved models\n",
        "loadTrainedModel = True\n",
        "actions = [\"Analyze model\", \"Show trained results\"]\n",
        "networkTypes = [\"CNN2\",\"CNN2_FFT\", \"CNN2_ENSEMBLE\", \"VGG\", \"VGG_ENSEMBLE\"]\n",
        "dataSets = [\"RML2016.10a_dict\", \"2016.04C.multisnr\"]\n",
        "for networkType in networkTypes:\n",
        "  print(networkType)\n",
        "  for dataSet in dataSets:\n",
        "    print(dataSet)\n",
        "    analyze_model_history()\n",
        "    show_trained_results()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yybbsiNyv9Bi",
        "cellView": "form"
      },
      "source": [
        "#@title Compare models\n",
        "title = f'model: {networkType}'\n",
        "\n",
        "old_data_path = f'{root}saved results/{networkType}, old data'\n",
        "new_data_path = f'{root}saved results/{networkType}, new data'\n",
        "fd = open(old_data_path, 'rb')\n",
        "acc_old = pickle.load(fd)\n",
        "fd.close()\n",
        "fd = open(new_data_path, 'rb')\n",
        "acc_new = pickle.load(fd)\n",
        "fd.close()\n",
        "\n",
        "plot_accuracy_graph([acc_old[-1], acc_new[-1]], save_fig=True, save_path=f'{root}saved results/snr graphs/{title}', legend=['old data','new data'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3yC5d7ZxPsf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}